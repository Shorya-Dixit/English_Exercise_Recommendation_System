{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stimuler Exercise Recommendation System\n",
    "\n",
    "### Specifics of the Problem Statement\n",
    "\n",
    "1. **Feedback Categories**: As users converse with the app, feedback is given to each user utterance on 4 broad level categories - Grammar, Vocabulary, Pronunciation, and the content/fluency of speech. Scores and detailed feedback analysis are provided on each of these parts.\n",
    "\n",
    "2. **Historical Data**: Historical data of each user’s past mistakes is used to suggest practice exercises. Assume the historical data of each user is available in a convenient format for building ML algorithms. Clarify data format assumptions in the final summary report.\n",
    "\n",
    "3. **Exercise Recommendation Flow**: \n",
    "   - User starts a new conversation with the app.\n",
    "   - At each user’s utterance, they are evaluated on the 4 categories (Grammar, Vocabulary, Pronunciation, Fluency/Content).\n",
    "   - Dynamically decide whether to show the user a practice exercise or not (this algorithm already exists).\n",
    "   - If a practice exercise is to be shown, generate the exercise based on the user’s past most common errors.\n",
    "\n",
    "4. **Exercise Generation**: \n",
    "   - Decide whether the exercise should be for grammar, vocabulary, pronunciation, or content/fluency.\n",
    "   - Generate the content of the practice exercise, which can be multimodal depending on the user’s demographics.\n",
    "\n",
    "### Proposed Solution\n",
    "\n",
    "#### Data Format Assumptions\n",
    "\n",
    "- **User Data**: Contains demographic information such as country, age, English proficiency level, and interests.\n",
    "- **Historical Data**: Contains past mistakes categorized into Grammar, Vocabulary, Pronunciation, and Fluency/Content, along with timestamps and context of errors.\n",
    "\n",
    "#### Approach\n",
    "\n",
    "1. **User Onboarding**: During onboarding, demographic information and user interests are collected. This information helps personalize the user experience by tailoring exercises to their background and preferences.\n",
    "\n",
    "2. **Error Tracking**: User errors are continuously tracked and categorized into four main categories: Grammar, Vocabulary, Pronunciation, and Fluency/Content. This historical data is crucial for identifying patterns and common mistakes, which are used to generate relevant practice exercises.\n",
    "\n",
    "3. **Dynamic Exercise Recommendation**:\n",
    "   - **Identify Common Mistakes**: Historical error data is analyzed to identify the most frequent mistakes made by the user. This helps focus on areas where the user needs the most improvement.\n",
    "   - **Decide Exercise Type**: Based on the identified common mistakes, the system decides whether the exercise should focus on grammar, vocabulary, pronunciation, or fluency.\n",
    "   - **Personalize Exercise Content**: The content of the practice exercise is generated based on user demographics and interests. For example, culturally relevant content such as popular sitcoms or anime is used to make the exercises more engaging and relatable.\n",
    "\n",
    "4. **Cold Start Scenario**: For new users, demographic information and initial interactions are used to make educated guesses about potential areas of improvement and interests. This can involve asking explicit questions during onboarding or using implicit feedback from initial conversations.\n",
    "\n",
    "### Measures\n",
    "\n",
    "To evaluate the effectiveness of the exercise recommendation system, the following measures can be used:\n",
    "\n",
    "- **User Engagement**: Measure the time spent on exercises and the number of exercises completed. Higher engagement indicates that the exercises are relevant and interesting to the users.\n",
    "- **Improvement in Scores**: Track the improvement in user scores over time in the four categories (Grammar, Vocabulary, Pronunciation, Fluency/Content). This helps in assessing whether the exercises are helping users improve their language skills.\n",
    "- **User Satisfaction**: Collect feedback from users about the exercises. This can be done through surveys or in-app feedback mechanisms. High satisfaction scores indicate that users find the exercises helpful and enjoyable.\n",
    "\n",
    "#### Metrics\n",
    "\n",
    "To evaluate user performance, the following metrics are used:\n",
    "\n",
    "- **Speech Rate**: Measures the number of words spoken per minute. A balanced speech rate indicates good fluency.\n",
    "- **Pause Ratio**: Measures the ratio of pauses to speech. A lower pause ratio indicates better fluency and confidence.\n",
    "- **Zero Crossing Rate**: Measures the rate at which the speech signal changes sign. A lower zero crossing rate indicates smoother speech and better pronunciation.\n",
    "\n",
    "#### Scores\n",
    "\n",
    "- **Vocabulary Score**: Assesses the richness of the user's vocabulary while also accounting for spelling accuracy. A higher score indicates a more extensive vocabulary with fewer spelling mistakes.\n",
    "- **Grammar Score**: Assesses the correctness of the user's grammar. A higher score indicates fewer grammatical errors.\n",
    "- **Pronunciation Score**: Evaluates the clarity and accuracy of the user's pronunciation. A higher score indicates better pronunciation.\n",
    "- **Fluency Score**: Assesses the coherence and relevance of the user's speech. A higher score indicates better fluency and content quality.\n",
    "\n",
    "#### Groq\n",
    "\n",
    "Groq is used to leverage the capabilities of large language models (LLMs) for generating personalized and contextually relevant practice exercises. The LLM model used in this system is llama3-70b-8192, which provides advanced natural language understanding and generation capabilities. Groq's architecture allows for efficient interaction with the LLM, enabling the system to generate high-quality exercises in real-time.\n",
    "\n",
    "#### LanguageTool API\n",
    "\n",
    "The LanguageTool API is used for grammar and spelling checks. It provides detailed feedback on grammatical errors, helping users improve their writing and speaking skills. The API is integrated into the system to analyze user utterances and provide real-time feedback on grammar and vocabulary.\n",
    "\n",
    "#### Summary\n",
    "\n",
    "This solution outlines a dynamic exercise recommendation system for Stimuler, leveraging user demographic information and historical error data to personalize practice exercises. The approach ensures continuous adaptation to user interests and addresses the cold start scenario for new users effectively. By focusing on user engagement, improvement in scores, and user satisfaction, the system aims to provide a tailored and effective learning experience for users from diverse backgrounds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assumed Data Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User Profile Data\n",
    "user_profile = {\n",
    "    'country': 'Japan',\n",
    "    'age_band': '20s',  # e.g., 10s, 20s, 30s\n",
    "    'proficiency_level': 'intermediate',  # e.g., beginner, intermediate, advanced\n",
    "    'interests': ['anime', 'sitcoms'],  # e.g., anime, movies, sports, sitcoms\n",
    "    'preferred_content_types': ['fill-in-the-blanks', 'memes']  # e.g., memes, fill-in-the-blanks, interactive quizzes, audio exercises\n",
    "}\n",
    "\n",
    "# User Performance Data\n",
    "user_performance = {\n",
    "    'grammar_score': 70,   # out of 100\n",
    "    'vocabulary_score': 72,\n",
    "    'pronunciation_score': 95,\n",
    "    'fluency_score': 90,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import time\n",
    "import whisper\n",
    "import scipy.io.wavfile as wav\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioRecorder:\n",
    "    def __init__(self, sample_rate=16000):\n",
    "        self.sample_rate = sample_rate\n",
    "        \n",
    "    def record_audio(self, duration):\n",
    "        \"\"\"Record audio for specified duration\"\"\"\n",
    "        print(f\"Recording for {duration} seconds...\")\n",
    "\n",
    "        recording = sd.rec(\n",
    "            int(duration * self.sample_rate),\n",
    "            samplerate=self.sample_rate,\n",
    "            channels=1,\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        for i in range(duration):\n",
    "            print(f\"Recording: {i+1}/{duration} seconds\")\n",
    "            time.sleep(1)\n",
    "            \n",
    "        sd.wait()\n",
    "        print(\"Recording finished!\")\n",
    "        return recording.flatten()\n",
    "    \n",
    "    def save_audio(self, audio_data, filename=\"recording.wav\"):\n",
    "        \"\"\"Save audio to WAV file\"\"\"\n",
    "        wav.write(filename, self.sample_rate, (audio_data * 32767).astype(np.int16))\n",
    "        print(f\"Audio saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeechAnalyzer:\n",
    "    def __init__(self, model_size=\"base\", sample_rate=16000):\n",
    "\n",
    "        print(\"Loading Whisper model...\")\n",
    "        self.model = whisper.load_model(model_size)\n",
    "        self.sample_rate = sample_rate\n",
    "        print(\"Model loaded!\")\n",
    "\n",
    "    def analyze_speech(self, audio_data, duration):\n",
    "\n",
    "        try:\n",
    "\n",
    "            audio_data = whisper.pad_or_trim(audio_data)\n",
    "            \n",
    "            result = self.model.transcribe(audio_data)\n",
    "            \n",
    "            fluency = self._analyze_fluency(audio_data, result[\"text\"], duration)\n",
    "            pronunciation = self._analyze_pronunciation(audio_data)\n",
    "            \n",
    "            return {\n",
    "                \"transcription\": result[\"text\"],\n",
    "                \"language\": result[\"language\"],\n",
    "                \"fluency\": fluency,\n",
    "                \"pronunciation\": pronunciation\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error during analysis: {e}\")\n",
    "            return None\n",
    "\n",
    "    def _analyze_fluency(self, audio_data, text, duration):\n",
    "\n",
    "        try:\n",
    "            \n",
    "            # Detect speech regions using energy-based splitting\n",
    "            speech_intervals = librosa.effects.split(audio_data, top_db=30)\n",
    "            print(\"Speech Intervals:\")\n",
    "            print(speech_intervals)\n",
    "            total_speech_duration = sum((end - start) / self.sample_rate for start, end in speech_intervals)\n",
    "            \n",
    "            words = len(text.split())\n",
    "            speech_rate = words / duration if duration > 0 else 0\n",
    "            \n",
    "            total_pause_duration = duration - total_speech_duration\n",
    "            print(f\"Total Pause Duration: {total_pause_duration}\")\n",
    "            print(f\"Total Speech Duration: {total_speech_duration}\")\n",
    "            pause_ratio = total_pause_duration / duration if duration > 0 else 0\n",
    "\n",
    "            onset_env = librosa.onset.onset_strength(y=audio_data, sr=self.sample_rate)\n",
    "            onset_env = librosa.util.normalize(onset_env)\n",
    "            rhythm_regularity = 1.0 - np.std(onset_env) / (np.mean(onset_env) + 1e-6)\n",
    "            pace_consistency = 1.0 - np.std(np.diff(onset_env)) / (np.mean(onset_env) + 1e-6)\n",
    "\n",
    "            # Fluency Score Calculation (out of 100)\n",
    "            score = 100\n",
    "\n",
    "            # Speech Rate Penalty (Normal rate: 2-2.5 words per second)\n",
    "            if speech_rate < 2:  # Too slow\n",
    "                score -= (2 - speech_rate) * 5\n",
    "            elif speech_rate > 2.5:  # Too fast\n",
    "                score -= (speech_rate - 2.5) * 5\n",
    "\n",
    "            # Pause Ratio Penalty (Higher pause ratio means lower fluency)\n",
    "            score -= pause_ratio * 40  \n",
    "\n",
    "            score = max(0, min(100, score))\n",
    "            \n",
    "            return {\n",
    "                \"speech_rate\": float(speech_rate),  # Words per second\n",
    "                \"pause_ratio\": float(pause_ratio),  # Proportion of silence\n",
    "                \"rhythm_regularity\": float(rhythm_regularity),  \n",
    "                \"pace_consistency\": float(pace_consistency),  \n",
    "                \"fluency_score\": float(score)  # Fluency score out of 100\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error in fluency analysis: {e}\")\n",
    "            return None\n",
    "\n",
    "    def _analyze_pronunciation(self, audio_data) -> dict:\n",
    "\n",
    "        try:\n",
    "            mfcc = librosa.feature.mfcc(y=audio_data, sr=self.sample_rate, n_mfcc=13)\n",
    "            spectral_centroid = librosa.feature.spectral_centroid(y=audio_data, sr=self.sample_rate)\n",
    "            zero_crossing_rate = librosa.feature.zero_crossing_rate(audio_data)\n",
    "            \n",
    "            mfcc_variance = np.var(mfcc)\n",
    "            spectral_variance = np.var(spectral_centroid)\n",
    "            zcr_mean = np.mean(zero_crossing_rate)\n",
    "            \n",
    "            optimal_zcr_min = 0.02\n",
    "            optimal_zcr_max = 0.08\n",
    "            \n",
    "            # Pronunciation Score Calculation (out of 100)\n",
    "            score = 100\n",
    "\n",
    "            if zcr_mean < optimal_zcr_min:\n",
    "                score -= (optimal_zcr_min - zcr_mean) * 500  # Penalize low ZCR\n",
    "            elif zcr_mean > optimal_zcr_max:\n",
    "                score -= (zcr_mean - optimal_zcr_max) * 500  # Penalize high ZCR\n",
    "            \n",
    "            score = max(0, min(100, score))\n",
    "            \n",
    "            return {\n",
    "                \"mfcc_variance\": float(mfcc_variance),\n",
    "                \"spectral_variance\": float(spectral_variance),\n",
    "                \"zero_crossing_rate\": float(zcr_mean),\n",
    "                \"score\": float(score)\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error in pronunciation analysis: {e}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Whisper model...\n",
      "Model loaded!\n",
      "\n",
      "Starting recording...\n",
      "Recording for 8 seconds...\n",
      "Recording: 1/8 seconds\n",
      "Recording: 2/8 seconds\n",
      "Recording: 3/8 seconds\n",
      "Recording: 4/8 seconds\n",
      "Recording: 5/8 seconds\n",
      "Recording: 6/8 seconds\n",
      "Recording: 7/8 seconds\n",
      "Recording: 8/8 seconds\n",
      "Recording finished!\n",
      "Audio saved to last_recording.wav\n",
      "\n",
      "Analyzing speech...\n",
      "Speech Intervals:\n",
      "[[20480 48128]\n",
      " [60416 69632]\n",
      " [70144 94208]]\n",
      "Total Pause Duration: 4.192\n",
      "Total Speech Duration: 3.808\n",
      "\n",
      "Results:\n",
      "Transcription:  Hello, my name is Saurri Dixit. I am a last year student at IIT Jamo.\n",
      "Detected Language: en\n",
      "\n",
      "Pronunciation:\n",
      "mfcc_variance: 25791.00\n",
      "spectral_variance: 1638675.98\n",
      "zero_crossing_rate: 0.05\n",
      "score: 100.00\n",
      "\n",
      "Fluency:\n",
      "speech_rate: 1.88\n",
      "pause_ratio: 0.52\n",
      "rhythm_regularity: -2.86\n",
      "pace_consistency: -2.13\n",
      "fluency_score: 78.41\n"
     ]
    }
   ],
   "source": [
    "def analyze_voice_input(duration=5):\n",
    "    \"\"\"Record and analyze voice input\"\"\"\n",
    "    # Initialize recorder and analyzer\n",
    "    recorder = AudioRecorder()\n",
    "    analyzer = SpeechAnalyzer()\n",
    "    \n",
    "    try:\n",
    "        # Record audio\n",
    "        print(\"\\nStarting recording...\")\n",
    "        audio_data = recorder.record_audio(duration)\n",
    "        \n",
    "        # Save audio (optional)\n",
    "        recorder.save_audio(audio_data, \"last_recording.wav\")\n",
    "        \n",
    "        # Analyze speech\n",
    "        print(\"\\nAnalyzing speech...\")\n",
    "        results = analyzer.analyze_speech(audio_data, duration)\n",
    "        \n",
    "        # Display results\n",
    "        if results:\n",
    "            print(\"\\nResults:\")\n",
    "            print(f\"Transcription: {results['transcription']}\")\n",
    "            print(f\"Detected Language: {results['language']}\")\n",
    "            print(\"\\nPronunciation:\")\n",
    "            for metric, value in results['pronunciation'].items():\n",
    "                print(f\"{metric}: {value:.2f}\")\n",
    "            print(\"\\nFluency:\")\n",
    "            for metric, value in results['fluency'].items():\n",
    "                print(f\"{metric}: {value:.2f}\")\n",
    "            \n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "# Test the function\n",
    "analyze_voice_input(8) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Whisper model...\n",
      "Model loaded!\n",
      "\n",
      "Starting recording...\n",
      "Recording for 8 seconds...\n",
      "Recording: 1/8 seconds\n",
      "Recording: 2/8 seconds\n",
      "Recording: 3/8 seconds\n",
      "Recording: 4/8 seconds\n",
      "Recording: 5/8 seconds\n",
      "Recording: 6/8 seconds\n",
      "Recording: 7/8 seconds\n",
      "Recording: 8/8 seconds\n",
      "Recording finished!\n",
      "Audio saved to last_recording.wav\n",
      "\n",
      "Analyzing speech...\n",
      "Speech Intervals:\n",
      "[[ 4608 29696]\n",
      " [38400 86016]]\n",
      "Total Pause Duration: 3.4559999999999995\n",
      "Total Speech Duration: 4.5440000000000005\n",
      "\n",
      "Results:\n",
      "Transcription:  Hello my name is Saurati Kshed. I am a last year student at IIT Jamun.\n",
      "Detected Language: en\n",
      "\n",
      "Pronunciation:\n",
      "mfcc_variance: 31198.53\n",
      "spectral_variance: 1729574.73\n",
      "zero_crossing_rate: 0.05\n",
      "score: 100.00\n",
      "\n",
      "Fluency:\n",
      "speech_rate: 1.88\n",
      "pause_ratio: 0.43\n",
      "rhythm_regularity: -2.30\n",
      "pace_consistency: -1.56\n",
      "fluency_score: 82.09\n",
      "\n",
      "Pronunciation Score: 100.00\n",
      "Fluency Score: 82.09\n"
     ]
    }
   ],
   "source": [
    "def analyze_voice_input(duration=5):\n",
    "    \"\"\"Record and analyze voice input\"\"\"\n",
    "    # Initialize recorder and analyzer\n",
    "    recorder = AudioRecorder()\n",
    "    analyzer = SpeechAnalyzer()\n",
    "    \n",
    "    try:\n",
    "        # Record audio\n",
    "        print(\"\\nStarting recording...\")\n",
    "        audio_data = recorder.record_audio(duration)\n",
    "        \n",
    "        # Save audio (optional)\n",
    "        recorder.save_audio(audio_data, \"last_recording.wav\")\n",
    "        \n",
    "        # Analyze speech\n",
    "        print(\"\\nAnalyzing speech...\")\n",
    "        results = analyzer.analyze_speech(audio_data, duration)\n",
    "        \n",
    "        # Display results\n",
    "        if results:\n",
    "            print(\"\\nResults:\")\n",
    "            print(f\"Transcription: {results['transcription']}\")\n",
    "            print(f\"Detected Language: {results['language']}\")\n",
    "            print(\"\\nPronunciation:\")\n",
    "            for metric, value in results['pronunciation'].items():\n",
    "                print(f\"{metric}: {value:.2f}\")\n",
    "            print(\"\\nFluency:\")\n",
    "            for metric, value in results['fluency'].items():\n",
    "                print(f\"{metric}: {value:.2f}\")\n",
    "            \n",
    "            # Calculate scores\n",
    "            pronunciation_score = results['pronunciation']['score']\n",
    "            fluency_score = results['fluency']['fluency_score']\n",
    "            \n",
    "            return results['transcription'], pronunciation_score, fluency_score\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Test the function\n",
    "text, pronunciation_score, fluency_score = analyze_voice_input(8)\n",
    "print(f\"\\nPronunciation Score: {pronunciation_score:.2f}\")\n",
    "print(f\"Fluency Score: {fluency_score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "API_URL = \"https://api.languagetool.org/v2/check\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking: She go to school every day.\n",
      "Grammar Errors Found:\n",
      " - Word: go\n",
      "   Suggestion(s): goes, went\n",
      "   Message: The pronoun ‘She’ is usually used with a third-person or a past tense verb.\n",
      "\n",
      "\n",
      "Checking: He have been working since morning.\n",
      "Grammar Errors Found:\n",
      " - Word: have\n",
      "   Suggestion(s): has, had\n",
      "   Message: The pronoun ‘He’ is usually used with a third-person or a past tense verb.\n",
      "\n",
      "\n",
      "Checking: The cat is on table.\n",
      "No grammar errors found.\n",
      "\n",
      "Checking: I has a pen.\n",
      "Grammar Errors Found:\n",
      " - Word: has\n",
      "   Suggestion(s): have\n",
      "   Message: Possible agreement error — use the base form here.\n",
      "\n",
      "\n",
      "Checking: He had a week grasp of the subject and made severel vocab mistakez.\n",
      "Grammar Errors Found:\n",
      " - Word: week\n",
      "   Suggestion(s): weak\n",
      "   Message: Please check whether ‘weak’ (opposite of strong) might be the correct word here instead of ‘week’ (a period of seven days).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def check_grammar_errors(text, lang=\"en-US\"):\n",
    "\n",
    "    params = {\"text\": text, \"language\": lang}\n",
    "    response = requests.post(API_URL, data=params)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        matches = result.get(\"matches\", [])\n",
    "\n",
    "        grammar_errors = []\n",
    "        for match in matches:\n",
    "            rule_id = match.get(\"rule\", {}).get(\"id\", \"\")\n",
    "            message = match.get(\"message\", \"\")\n",
    "            word = match.get(\"context\", {}).get(\"text\", \"\")[match.get(\"offset\", 0): match.get(\"offset\", 0) + match.get(\"length\", 0)]\n",
    "            \n",
    "            # Exclude vocabulary errors like spelling and confused words\n",
    "            if \"MORFOLOGIK_RULE\" not in rule_id and \"CONFUSED_WORDS\" not in rule_id:\n",
    "                grammar_errors.append({\n",
    "                    \"word\": word,\n",
    "                    \"suggestions\": [sug[\"value\"] for sug in match.get(\"replacements\", [])],\n",
    "                    \"message\": message\n",
    "                })\n",
    "\n",
    "        return grammar_errors\n",
    "    else:\n",
    "        return {\"error\": f\"API request failed with status code {response.status_code}\"}\n",
    "\n",
    "sentences = [\n",
    "    \"She go to school every day.\",  \n",
    "    \"He have been working since morning.\",  \n",
    "    \"The cat is on table.\",  \n",
    "    \"I has a pen.\",  \n",
    "    \"He had a week grasp of the subject and made severel vocab mistakez.\"  \n",
    "]\n",
    "\n",
    "for sentence in sentences:\n",
    "    print(f\"\\nChecking: {sentence}\")\n",
    "    errors = check_grammar_errors(sentence)\n",
    "\n",
    "    if errors:\n",
    "        print(\"Grammar Errors Found:\")\n",
    "        for err in errors:\n",
    "            print(f\" - Word: {err['word']}\")\n",
    "            print(f\"   Suggestion(s): {', '.join(err['suggestions'])}\")\n",
    "            print(f\"   Message: {err['message']}\\n\")\n",
    "    else:\n",
    "        print(\"No grammar errors found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking: He had a week grasp of the subject and made severel vocab mistakez.\n",
      "Vocabulary Errors Found:\n",
      " - Word: everel \n",
      "   Suggestion(s): several, severe, severely, severed, severer\n",
      "   Message: Possible spelling mistake found.\n",
      "\n",
      " - Word: \n",
      "   Suggestion(s): mistake, mistaken, mistakes, mistaker\n",
      "   Message: Possible spelling mistake found.\n",
      "\n",
      "\n",
      "Checking: Their going to the store later.\n",
      "No vocabulary errors found.\n",
      "\n",
      "Checking: The effect of the medcine is immediate.\n",
      "Vocabulary Errors Found:\n",
      " - Word: medcine\n",
      "   Suggestion(s): medicine, med cine\n",
      "   Message: Possible spelling mistake found.\n",
      "\n",
      "\n",
      "Checking: He wrote a greate essay.\n",
      "Vocabulary Errors Found:\n",
      " - Word: greate\n",
      "   Suggestion(s): great, create, greater, grease, greats, grate, greave, Greater\n",
      "   Message: Possible spelling mistake found.\n",
      "\n",
      "\n",
      "Checking: This is a correct sentence.\n",
      "No vocabulary errors found.\n"
     ]
    }
   ],
   "source": [
    "def check_vocabulary_errors(text, lang=\"en-US\"):\n",
    "\n",
    "    params = {\"text\": text, \"language\": lang}\n",
    "    response = requests.post(API_URL, data=params)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        matches = result.get(\"matches\", [])\n",
    "\n",
    "        vocab_errors = []\n",
    "        for match in matches:\n",
    "            rule_id = match.get(\"rule\", {}).get(\"id\", \"\")\n",
    "            message = match.get(\"message\", \"\")\n",
    "            word = match.get(\"context\", {}).get(\"text\", \"\")[match.get(\"offset\", 0): match.get(\"offset\", 0) + match.get(\"length\", 0)]\n",
    "\n",
    "            # Only include vocabulary-related errors (spelling & misused words)\n",
    "            if \"MORFOLOGIK_RULE\" in rule_id or \"CONFUSED_WORDS\" in rule_id:\n",
    "                vocab_errors.append({\n",
    "                    \"word\": word,\n",
    "                    \"suggestions\": [sug[\"value\"] for sug in match.get(\"replacements\", [])],\n",
    "                    \"message\": message\n",
    "                })\n",
    "\n",
    "        return vocab_errors\n",
    "    else:\n",
    "        return {\"error\": f\"API request failed with status code {response.status_code}\"}\n",
    "    \n",
    "sentences = [\n",
    "    \"He had a week grasp of the subject and made severel vocab mistakez.\",  \n",
    "    \"Their going to the store later.\",  \n",
    "    \"The effect of the medcine is immediate.\",  \n",
    "    \"He wrote a greate essay.\",  \n",
    "    \"This is a correct sentence.\"  \n",
    "]\n",
    "\n",
    "for sentence in sentences:\n",
    "    print(f\"\\nChecking: {sentence}\")\n",
    "    errors = check_vocabulary_errors(sentence)\n",
    "\n",
    "    if errors:\n",
    "        print(\"Vocabulary Errors Found:\")\n",
    "        for err in errors:\n",
    "            print(f\" - Word: {err['word']}\")\n",
    "            print(f\"   Suggestion(s): {', '.join(err['suggestions'])}\")\n",
    "            print(f\"   Message: {err['message']}\\n\")\n",
    "    else:\n",
    "        print(\"No vocabulary errors found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_vocabulary_score(text, lang=\"en-US\"):\n",
    "\n",
    "    params = {\"text\": text, \"language\": lang}\n",
    "    response = requests.post(API_URL, data=params)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        matches = result.get(\"matches\", [])\n",
    "\n",
    "        vocab_errors = []\n",
    "        error_length = 0\n",
    "        total_length = len(text)\n",
    "\n",
    "        for match in matches:\n",
    "            rule_id = match.get(\"rule\", {}).get(\"id\", \"\")\n",
    "            message = match.get(\"message\", \"\")\n",
    "            word = match.get(\"context\", {}).get(\"text\", \"\")[match.get(\"offset\", 0): match.get(\"offset\", 0) + match.get(\"length\", 0)]\n",
    "\n",
    "            # Only include vocabulary-related errors (spelling & misused words)\n",
    "            if \"MORFOLOGIK_RULE\" in rule_id or \"CONFUSED_WORDS\" in rule_id:\n",
    "                vocab_errors.append({\n",
    "                    \"word\": word,\n",
    "                    \"suggestions\": [sug[\"value\"] for sug in match.get(\"replacements\", [])],\n",
    "                    \"message\": message\n",
    "                })\n",
    "                error_length += len(word)\n",
    "\n",
    "        score = 100 - (error_length / total_length) * 100\n",
    "\n",
    "        return score\n",
    "    else:\n",
    "        return {\"error\": f\"API request failed with status code {response.status_code}\"}\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89.55223880597015"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_vocabulary_score(\"He had a week grasp of the subject and made severel vocab mistakez.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_grammar_score(text, lang=\"en-US\"):\n",
    "\n",
    "    params = {\"text\": text, \"language\": lang}\n",
    "    response = requests.post(API_URL, data=params)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        matches = result.get(\"matches\", [])\n",
    "\n",
    "        grammar_errors = []\n",
    "        error_length = 0\n",
    "        total_length = len(text)\n",
    "\n",
    "        for match in matches:\n",
    "            rule_id = match.get(\"rule\", {}).get(\"id\", \"\")\n",
    "            message = match.get(\"message\", \"\")\n",
    "            word = match.get(\"context\", {}).get(\"text\", \"\")[match.get(\"offset\", 0): match.get(\"offset\", 0) + match.get(\"length\", 0)]\n",
    "            \n",
    "            # Exclude vocabulary errors like spelling and confused words\n",
    "            if \"MORFOLOGIK_RULE\" not in rule_id and \"CONFUSED_WORDS\" not in rule_id:\n",
    "                grammar_errors.append({\n",
    "                    \"word\": word,\n",
    "                    \"suggestions\": [sug[\"value\"] for sug in match.get(\"replacements\", [])],\n",
    "                    \"message\": message\n",
    "                })\n",
    "                error_length += len(word)\n",
    "\n",
    "        score = 100 - (error_length / total_length) * 100\n",
    "\n",
    "        return score\n",
    "    else:\n",
    "        return {\"error\": f\"API request failed with status code {response.status_code}\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94.02985074626866"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_grammar_score(\"He had a week grasp of the subject and made severel vocab mistakez.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, clear_output\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your input:\n",
      "He had a week grasp of the subject and made severel vocab mistakez.\n",
      "\n",
      "Scores:\n",
      "Vocabulary Score: 89.55\n",
      "Grammar Score: 94.03\n",
      "\n",
      "Updated Performance:\n",
      "Average Vocabulary Score: 80.78\n",
      "Average Grammar Score: 82.01\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9009a5de9184bda99a32c533db64a49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Try Again', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def App():\n",
    "    def on_input_type_change(change):\n",
    "        clear_output()\n",
    "        if change['new'] == 'Voice Input':\n",
    "            get_voice_input()\n",
    "        else:\n",
    "            get_text_input()\n",
    "    \n",
    "    def get_voice_input():\n",
    "        voice_input, pronunciation_score, fluency_score = analyze_voice_input(8)\n",
    "\n",
    "        # Update user_performance dictionary\n",
    "        if 'pronunciation_score' in user_performance:\n",
    "            user_performance['pronunciation_score'] = (user_performance['pronunciation_score'] + pronunciation_score) / 2\n",
    "        else:\n",
    "            user_performance['pronunciation_score'] = pronunciation_score\n",
    "            \n",
    "        if 'fluency_score' in user_performance:\n",
    "            user_performance['fluency_score'] = (user_performance['fluency_score'] + fluency_score) / 2\n",
    "        else:\n",
    "            user_performance['fluency_score'] = fluency_score\n",
    "\n",
    "        print(\"Your input:\")\n",
    "        print(voice_input)\n",
    "        print(\"\\nScores:\")\n",
    "        print(f\"Pronunciation Score: {pronunciation_score:.2f}\")\n",
    "        print(f\"Fluency Score: {fluency_score:.2f}\")\n",
    "        print(\"\\nUpdated Performance:\")\n",
    "        print(f\"Average Pronunciation Score: {user_performance['pronunciation_score']:.2f}\")\n",
    "        print(f\"Average Fluency Score: {user_performance['fluency_score']:.2f}\")\n",
    "    \n",
    "    def calculate_and_update_scores(text):\n",
    "        # Calculate new scores\n",
    "        vocab_score = calculate_vocabulary_score(text)\n",
    "        grammar_score = calculate_grammar_score(text)\n",
    "        \n",
    "        # Update user_performance dictionary\n",
    "        if 'vocabulary_score' in user_performance:\n",
    "            user_performance['vocabulary_score'] = (user_performance['vocabulary_score'] + vocab_score) / 2\n",
    "        else:\n",
    "            user_performance['vocabulary_score'] = vocab_score\n",
    "            \n",
    "        if 'fluency_score' in user_performance:\n",
    "            user_performance['grammar_score'] = (user_performance['grammar_score'] + grammar_score) / 2\n",
    "        else:\n",
    "            user_performance['grammar_score'] = grammar_score\n",
    "            \n",
    "        return vocab_score, grammar_score\n",
    "    \n",
    "    def get_text_input():\n",
    "        text_input = widgets.Text(\n",
    "            placeholder='Type your input here...',\n",
    "            description='Input:',\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "        \n",
    "        submit_button = widgets.Button(description='Submit')\n",
    "        \n",
    "        def on_submit(b):\n",
    "            vocab_score, grammar_score = calculate_and_update_scores(text_input.value)\n",
    "            display_output(text_input.value, vocab_score, grammar_score)\n",
    "        \n",
    "        submit_button.on_click(on_submit)\n",
    "        \n",
    "        display(widgets.VBox([text_input, submit_button]))\n",
    "    \n",
    "    def display_output(text, vocab_score, grammar_score):\n",
    "        clear_output()\n",
    "        print(\"Your input:\")\n",
    "        print(text)\n",
    "        print(\"\\nScores:\")\n",
    "        print(f\"Vocabulary Score: {vocab_score:.2f}\")\n",
    "        print(f\"Grammar Score: {grammar_score:.2f}\")\n",
    "        print(\"\\nUpdated Performance:\")\n",
    "        print(f\"Average Vocabulary Score: {user_performance['vocabulary_score']:.2f}\")\n",
    "        print(f\"Average Grammar Score: {user_performance['grammar_score']:.2f}\")\n",
    "        \n",
    "        retry_button = widgets.Button(description='Try Again')\n",
    "        \n",
    "        def on_retry(b):\n",
    "            clear_output()\n",
    "            start_app()\n",
    "        \n",
    "        retry_button.on_click(on_retry)\n",
    "        display(retry_button)\n",
    "    \n",
    "    def start_app():\n",
    "        input_type = widgets.RadioButtons(\n",
    "            options=['Text Input', 'Voice Input'],\n",
    "            description='Choose input type:',\n",
    "            style={'description_width': 'initial'},\n",
    "            value='Text Input'  # Set default value\n",
    "        )\n",
    "        \n",
    "        input_type.observe(on_input_type_change, names='value')\n",
    "        display(input_type)\n",
    "\n",
    "        get_text_input()\n",
    "    \n",
    "    start_app()\n",
    "\n",
    "App()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Whisper model...\n",
      "Model loaded!\n",
      "\n",
      "Starting recording...\n",
      "Recording for 8 seconds...\n",
      "Recording: 1/8 seconds\n",
      "Recording: 2/8 seconds\n",
      "Recording: 3/8 seconds\n",
      "Recording: 4/8 seconds\n",
      "Recording: 5/8 seconds\n",
      "Recording: 6/8 seconds\n",
      "Recording: 7/8 seconds\n",
      "Recording: 8/8 seconds\n",
      "Recording finished!\n",
      "Audio saved to last_recording.wav\n",
      "\n",
      "Analyzing speech...\n",
      "Speech Intervals:\n",
      "[[11264 36864]\n",
      " [46592 70144]\n",
      " [70656 86528]]\n",
      "Total Pause Duration: 3.936\n",
      "Total Speech Duration: 4.064\n",
      "\n",
      "Results:\n",
      "Transcription:  Hello my name is Sohra Dixip, I am a last-year student at IIT Zomo.\n",
      "Detected Language: en\n",
      "\n",
      "Pronunciation:\n",
      "mfcc_variance: 29939.19\n",
      "spectral_variance: 1722176.22\n",
      "zero_crossing_rate: 0.05\n",
      "score: 100.00\n",
      "\n",
      "Fluency:\n",
      "speech_rate: 1.75\n",
      "pause_ratio: 0.49\n",
      "rhythm_regularity: -2.37\n",
      "pace_consistency: -1.59\n",
      "fluency_score: 79.07\n",
      "Your input:\n",
      " Hello my name is Sohra Dixip, I am a last-year student at IIT Zomo.\n",
      "\n",
      "Scores:\n",
      "Pronunciation Score: 100.00\n",
      "Fluency Score: 79.07\n",
      "\n",
      "Updated Performance:\n",
      "Average Pronunciation Score: 98.75\n",
      "Average Fluency Score: 81.05\n"
     ]
    }
   ],
   "source": [
    "App()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import groq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Exercise:\n",
      "\n",
      "Here's a fun exercise to help improve vocabulary skills, tailored to a Japanese learner in their 20s with an intermediate proficiency level:\n",
      "\n",
      "**Anime-themed Vocabulary Building Exercise**\n",
      "\n",
      "**Fill in the Blanks: \"Anime Characters' Catchphrases\"**\n",
      "\n",
      "Complete the sentences with the correct vocabulary words. Choose from the options provided.\n",
      "\n",
      "1. In the anime \"Naruto,\" Naruto Uzumaki's catchphrase is \"Believe it!\" which means to have _______________________ in someone or something.\n",
      "\n",
      "A) confidence\n",
      "B) skepticism\n",
      "C) nostalgia\n",
      "D) empathy\n",
      "\n",
      "Answer: A) confidence\n",
      "\n",
      "2. Light Yagami from \"Death Note\" is known for his _______________________ nature, always staying one step ahead of L.\n",
      "\n",
      "A) meticulous\n",
      "B) spontaneous\n",
      "C) gullible\n",
      "D) naive\n",
      "\n",
      "Answer: A) meticulous\n",
      "\n",
      "3. In \"One Punch Man,\" Saitama's _______________________ attitude towards his superhero life is both relatable and hilarious.\n",
      "\n",
      "A) nonchalant\n",
      "B) enthusiastic\n",
      "C) sarcastic\n",
      "D) melancholic\n",
      "\n",
      "Answer: A) nonchalant\n",
      "\n",
      "**Meme Time!**\n",
      "\n",
      "Identify the correct vocabulary word to complete the meme. Choose from the options provided.\n",
      "\n",
      "[Image of a meme with a character from \"Haikyuu!!\" looking frustrated]\n",
      "\n",
      "When you finally understand the volleyball strategy, but your teammates are still _______________________.\n",
      "\n",
      "A) oblivious\n",
      "B) vigilant\n",
      "C) ecstatic\n",
      "D) lethargic\n",
      "\n",
      "Answer: A) oblivious\n",
      "\n",
      "[Image of a meme with a character from \"The Office\" (US) looking surprised]\n",
      "\n",
      "When you realize you've been using the wrong Japanese phrase in your anime fanfic for months and it's been _______________________.\n",
      "\n",
      "A) ambiguous\n",
      "B) erroneous\n",
      "C) nostalgic\n",
      "D) euphoric\n",
      "\n",
      "Answer: B) erroneous\n",
      "\n",
      "**Sitcom-themed Vocabulary Building Exercise**\n",
      "\n",
      "**Fill in the Blanks: \"Friends' Favorite Phrases\"**\n",
      "\n",
      "Complete the sentences with the correct vocabulary words. Choose from the options provided.\n",
      "\n",
      "1. Joey Tribbiani's love for food is _______________________, often leading to comedic moments in the show.\n",
      "\n",
      "A) insatiable\n",
      "B) fastidious\n",
      "C) hesitant\n",
      "D) apathetic\n",
      "\n",
      "Answer: A) insatiable\n",
      "\n",
      "2. Monica Geller's _______________________ nature makes her a cleanliness freak and a great chef.\n",
      "\n",
      "A) meticulous\n",
      "B) carefree\n",
      "C) spontaneous\n",
      "D) laid-back\n",
      "\n",
      "Answer: A) meticulous\n",
      "\n",
      "3. Chandler Bing's _______________________ sense of humor often helps his friends in awkward situations.\n",
      "\n",
      "A) sarcastic\n",
      "B) ironic\n",
      "C) witty\n",
      "D) solemn\n",
      "\n",
      "Answer: A) sarcastic\n",
      "\n",
      "**Bonus Question!**\n",
      "\n",
      "What does the phrase \"Break a leg!\" mean in English?\n",
      "\n",
      "A) Good luck!\n",
      "B) You're going to fail!\n",
      "C) Take a break!\n",
      "D) Be careful!\n",
      "\n",
      "Answer: A) Good luck!\n",
      "\n",
      "I hope you enjoyed this exercise! Let me know if you need more help or have any questions.\n"
     ]
    }
   ],
   "source": [
    "def get_lowest_skill(user_performance):\n",
    "    return min(user_performance, key=user_performance.get)\n",
    "\n",
    "def generate_prompt(user_profile, user_performance):\n",
    "    lowest_skill = get_lowest_skill(user_performance)\n",
    "    interests = ', '.join(user_profile['interests'])\n",
    "    preferred_formats = ', '.join(user_profile['preferred_content_types'])\n",
    "    proficiency_level = user_profile['proficiency_level']\n",
    "\n",
    "    \n",
    "    prompt = (\n",
    "        f\"Generate an English language exercise for a user from {user_profile['country']} \"\n",
    "        f\"in their {user_profile['age_band']} with an {proficiency_level} proficiency level. \"\n",
    "        f\"The user prefers formats like {preferred_formats}. \"\n",
    "        f\"The exercise should focus on improving {lowest_skill}, as it is their weakest area. \"\n",
    "        f\"Make the exercise engaging by including references to {interests}. \"\n",
    "        f\"Ensure the content is suitable for an {proficiency_level} learner.\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "def generate_exercise(prompt):\n",
    "    client = groq.Client(api_key=\"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\")  \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"llama3-70b-8192\",  \n",
    "        messages=[{\"role\": \"system\", \"content\": \"You are an expert English teacher.\"},\n",
    "                  {\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    prompt = generate_prompt(user_profile, user_performance)\n",
    "    exercise = generate_exercise(prompt)\n",
    "    \n",
    "    print(\"Generated Exercise:\\n\")\n",
    "    print(exercise)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
